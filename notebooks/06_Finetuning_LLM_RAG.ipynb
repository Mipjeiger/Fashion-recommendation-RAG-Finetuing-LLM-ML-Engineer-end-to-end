{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45690c80",
   "metadata": {},
   "source": [
    "## RAG Finetuning LLM\n",
    "### * **Retrieval based on PDF relevant about fashion recommendation to Vector Database**\n",
    "### * **Integrating LLM-based attribute aware context with fine-grained fashion retrieval. For each attribute in the query the LLM first generates a detailed attribute-aware context for enriching attribute representations with commonsense business insight requirements.**\n",
    "### * **The attribute embeddings, enriched with their attribute- aware context, form a conditional query vector that guides the retrieval process, interacting with image patches to focus on relevant regions that match the specified attributes.**\n",
    "### * **Prompt generation training strategies to enhance its capacity for delivering personalized fashion advice while retaining essential domain knowledge.**\n",
    "### * **Generative images AI Engineering.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce9a81",
   "metadata": {},
   "source": [
    "# LLM Strategies\n",
    "### These strategies, as reflected in the designed prompts, \n",
    "### Ensure that the LLM not only retains its core language processing capabilities but is also finely tuned to analyze and address fashion-related queries with enhanced precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c202f",
   "metadata": {},
   "source": [
    "## Load & chunk PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127674f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pypdf\")\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Text chunking\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# call the functions\n",
    "pdf_text = load_pdf_text(\"../data/pdf/fashion recommendation LLM.pdf\")\n",
    "pdf_chunks = chunk_text(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2401b736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Integrating Domain Knowledge into Large Language Models for\\nEnhanced Fashion Recommendations\\nZhan Shi∗∗\\naria2@scu.edu\\nSanta Clara University\\nSanta Clara, USA\\nShanglin Yang†\\nkudoysl@gmail.com\\nABSTRACT\\nFashion, deeply rooted in sociocultural dynamics, evolves as individ-\\nuals emulate styles popularized by influencers and iconic figures. In\\nthe quest to replicate such refined tastes using artificial intelligence,\\ntraditional fashion ensemble methods have primarily used super-\\nvised learning to imit',\n",
       " 'intelligence,\\ntraditional fashion ensemble methods have primarily used super-\\nvised learning to imitate the decisions of style icons, which falter\\nwhen faced with distribution shifts, leading to style replication dis-\\ncrepancies triggered by slight variations in input. Meanwhile, large\\nlanguage models (LLMs) have become prominent across various\\nsectors, recognized for their user-friendly interfaces, strong con-\\nversational skills, and advanced reasoning capabilities. To address\\nthese challenges,',\n",
       " 's, strong con-\\nversational skills, and advanced reasoning capabilities. To address\\nthese challenges, we introduce the Fashion Large Language Model\\n(FLLM), which employs auto-prompt generation training strate-\\ngies to enhance its capacity for delivering personalized fashion\\nadvice while retaining essential domain knowledge. Additionally,\\nby integrating a retrieval augmentation technique during inference,\\nthe model can better adjust to individual preferences. Our results\\nshow that this approach su',\n",
       " 'rence,\\nthe model can better adjust to individual preferences. Our results\\nshow that this approach surpasses existing models in accuracy,\\ninterpretability, and few-shot learning capabilities.\\nCCS CONCEPTS\\n•Information systems → Novelty in information retrieval.\\nKEYWORDS\\nFashion Recommendation, LLM, RAG\\nACM Reference Format:\\nZhan Shi and Shanglin Yang. 2024. Integrating Domain Knowledge into\\nLarge Language Models for Enhanced Fashion Recommendations. In Pro-\\nceedings of Make sure to enter the corr',\n",
       " 'anguage Models for Enhanced Fashion Recommendations. In Pro-\\nceedings of Make sure to enter the correct conference title from your rights\\nconfirmation email (RecSys 2024). ACM, New York, NY, USA, 5 pages. https:\\n//doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nFashion plays a significant role in various aspects of life, including\\nsocial and cultural identity. People often imitate styles suggested\\nby fashion experts, icons, or Key Opinion Leaders (KOLs) from\\n∗Both authors contributed equally to this rese',\n",
       " 'on experts, icons, or Key Opinion Leaders (KOLs) from\\n∗Both authors contributed equally to this research.\\n†Both authors contributed equally to this research.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than t',\n",
       " 'd the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nRecSys 2024, October 14–18, 2024, Bari, Italy\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/1',\n",
       " 'yright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nFigure 1: Fashion item recommendations can differ signif-\\nicantly based on various occasions, styles, and attributes.\\n(a) Traditional methods (red box), trained solely on fixed\\ndatasets using sequence models, are constrained to aligning\\nwith the training data and struggle to adapt to new prefer-\\nence distributions. In contrast, our (b) LLM methods (',\n",
       " 'ing data and struggle to adapt to new prefer-\\nence distributions. In contrast, our (b) LLM methods (green\\nbox) dynamically integrate diverse contexts into the prompt.\\nLeveraging the inference capabilities of large models, our\\nmethods produce results under a wide range of conditions.\\nsocial media. In the rapidly evolving fashion technology sector, cre-\\nating advanced, personalized recommendation systems is a major\\nacademic and commercial endeavor. This research introduces an\\ninnovative approach u',\n",
       " 'stems is a major\\nacademic and commercial endeavor. This research introduces an\\ninnovative approach using Large Language Models (LLMs) to trans-\\nform fashion recommendations. Unlike traditional methods relying\\non static datasets and supervised learning, our generative model\\nleverages LLMs to provide personalized, flexible fashion advice.\\nThis approach overcomes previous limitations and offers a deeper\\nconnection with individual preferences, reflecting the dynamic na-\\nture of fashion. Fashion tren',\n",
       " 'per\\nconnection with individual preferences, reflecting the dynamic na-\\nture of fashion. Fashion trends change rapidly, posing challenges\\nfor recommendation systems to forecast and adapt to personal pref-\\nerences. Conventional systems, though somewhat successful in\\npredicting item compatibility and ensuring stylistic consistency,\\noften fail to cater to diverse individual styles and rely heavily on\\nstatic datasets, limiting their relevance over time.\\nTo address these challenges, our study presents',\n",
       " 'static datasets, limiting their relevance over time.\\nTo address these challenges, our study presents an innovative so-\\nlution using LLMs’ generative and adaptive strengths [20], offering\\nnew methods for generating recommendations that include items,\\noccasions, and styles. We propose a new training paradigm for\\narXiv:2502.15696v1  [cs.CL]  3 Jan 2025\\nRecSys 2024, October 14–18, 2024, Bari, Italy Zhan Shi and Shanglin Yang\\nLLMs, starting with fashion-specific data to retain core knowledge\\nand prog',\n",
       " 'an Shi and Shanglin Yang\\nLLMs, starting with fashion-specific data to retain core knowledge\\nand progressing through specialized training phases. This includes\\na foundational phase with ’FIIB’ data, a combined phase of style\\nand ’FIIB’ data, and the development of a custom dataset for final\\nrefinement.\\nThe paper is structured as follows: We first explore the current\\nlandscape of fashion recommendation systems, highlighting their\\nlimitations and the potential of LLMs. We then detail our novel\\nappr',\n",
       " 'ion systems, highlighting their\\nlimitations and the potential of LLMs. We then detail our novel\\napproach, including the architecture of our LLM-based system, data\\nsources, training procedure, and the use of retrieval-augmented\\ngeneration. Following this, we describe our experimental design,\\nevaluation metrics, and empirical study results, demonstrating our\\nmodel’s superior performance in accuracy, adaptability, and user\\nsatisfaction. We conclude with a discussion of the broader implica-\\ntions an',\n",
       " ' adaptability, and user\\nsatisfaction. We conclude with a discussion of the broader implica-\\ntions and future research directions. Our key contributions are:\\n• Reconceptualizing fashion recommendation as a generative\\nchallenge using LLMs.\\n• Incorporating domain fine-tuning and retrieval-augmented\\ngeneration to enable the model to learn new fashion knowl-\\nedge while preserving domain expertise.\\n• Demonstrating the model’s performance through experi-\\nments, highlighting improved accuracy, impressiv',\n",
       " 'monstrating the model’s performance through experi-\\nments, highlighting improved accuracy, impressive few-shot\\nlearning capabilities, and adaptability to different contexts.\\n2 RELATED WORK\\nIn recent years, the use of deep learning neural networks, high-\\nlighted by [ 3], has drawn significant attention in various tasks\\nwithin the fashion industry. These tasks include category and at-\\ntribute classification ([13]), trend forecasting ([1], popularity pre-\\ndiction ([14]), and the development of fash',\n",
       " 'fication ([13]), trend forecasting ([1], popularity pre-\\ndiction ([14]), and the development of fashion recommendation\\nsystems ([6], [16]), particularly in outfit recommendation. It’s cru-\\ncial to determine which garments go well together to recommend\\ncomplete outfits that are compatible and cohesive.\\nInitial attempts to address outfit compatibility treated it as a\\nseries of pairwise comparisons among all garments in an outfit,\\nas explored by [17]. These pairwise-based methods used Siamese\\nnetwo',\n",
       " 'mong all garments in an outfit,\\nas explored by [17]. These pairwise-based methods used Siamese\\nnetworks ([19]) and triplet loss networks, with either type-aware\\nembeddings ([18]) or similarity-aware embeddings ([18]). In con-\\ntrast, some researchers have aimed to capture a holistic view of\\noutfit-level representations through bidirectional LSTMs ([5]) or\\ngraph neural networks ([4], [10]). Some studies also have shifted to\\nattention-based methods, including the Transformer architecture\\nfor person',\n",
       " 'dies also have shifted to\\nattention-based methods, including the Transformer architecture\\nfor personalized outfit recommendations and complementary item\\nretrieval ([12], [21], [15]).\\nResearchers have increasingly turned to Large Language Models\\n(LLMs) for their outstanding text generation capabilities, particu-\\nlarly for data augmentation ([7], [9]). [2], for instance, used LLMs\\nto create multimodal datasets that blend language and images for\\ninstruction-following tasks. Their method, which invo',\n",
       " 'al datasets that blend language and images for\\ninstruction-following tasks. Their method, which involved tuning\\nthe instructions based on this data, significantly improved both\\nvision and language comprehension.\\nIn the domain of personalized recommendation systems, LLMs\\nhave also played a crucial role. [ 23] employed LLMs to generate\\nuser profiles by incorporating behaviors like clicks, purchases, and\\nratings. These profiles, when combined with the history of user\\ninteractions and potential item',\n",
       " ' and\\nratings. These profiles, when combined with the history of user\\ninteractions and potential items, were key in formulating the final\\nrecommendation prompt. LLMs were then applied to determine the\\nlikelihood of user-item interactions from this prompt. In a similar\\nvein, [11] developed a technique that leverages LLMs to merge\\nreasoning about user preferences with factual knowledge of items.\\nHowever, our research explores a new area: there is no established\\nwork on using LLMs in fashion recomme',\n",
       " 'ver, our research explores a new area: there is no established\\nwork on using LLMs in fashion recommendation. Therefore, our\\nstudy establishes a novel baseline for LLM application in the fashion\\nrecommendation field.\\n3 METHODOLOGY\\n3.1 Problem Formulation\\nFigure 2 illustrates our sophisticated fashion recommendation sys-\\ntem, which utilizes a finely-tuned Large Language Model (LLM)\\nintegrated with domain-specific retrieval processes. This system,\\noptimized with a dedicated fashion dataset, is adep',\n",
       " 'omain-specific retrieval processes. This system,\\noptimized with a dedicated fashion dataset, is adept at parsing the\\ncomplexities of fashion nuances. It employs the LLM to create spe-\\ncific prompts that trigger searches within a comprehensive fashion\\nproduct dataset, and a dual-component mechanism retrieves vital\\ndomain context by merging industry trends and design principles\\nwith user preferences to tailor recommendations.\\nThe system processes both user and system-generated text via\\ntokenizers,',\n",
       " ' to tailor recommendations.\\nThe system processes both user and system-generated text via\\ntokenizers, converting them into tokens suitable for the LLM. The\\nmodel then analyzes these inputs to generate customized recom-\\nmendations, such as suggesting \"floral-print pants\" among various\\nitem choices. Fashion-related documents are processed into em-\\nbeddings through a sentence transformer and stored in a vector\\ndatabase. These embeddings are then matched with user query\\nembeddings to further refine t',\n",
       " ' a vector\\ndatabase. These embeddings are then matched with user query\\nembeddings to further refine the recommendations.\\nUltimately, the LLM delivers sophisticated responses to user\\nqueries, resulting in either well-coordinated outfit recommenda-\\ntions or detailed fashion advice. This ensures both contextual rel-\\nevance and personalization. By integrating system prompts, item\\ndescription and processed contextual data, the system achieves\\nprecise and personalized fashion recommendations.\\n3.2 Fintu',\n",
       " 'sed contextual data, the system achieves\\nprecise and personalized fashion recommendations.\\n3.2 Fintuning LLM with auto-prompts\\ngeneration\\nThe diagram illustrated in Figure 3 delineates the comprehensive\\ntraining and refinement process for a specialized Fashion Large\\nLanguage Model (FLLM), which is tasked with dispensing tailored\\nfashion advice. Initially, the FLLM is acquainted with Polyvore out-\\nfit training data. This data encompasses two primary types: binary\\nquestion data, which prompts the ',\n",
       " 'fit training data. This data encompasses two primary types: binary\\nquestion data, which prompts the model to discern the appropri-\\nateness of an outfit, and Fill-in-the-Blank (FITB) data.\\nTo enhance the robustness and proficiency of our Fashion Large\\nLanguage Model (FLLM), we employ a dual-strategy approach in\\ngenerating training data. Initially, we source training data directly\\nfrom existing datasets. Additionally, we utilize the advanced capa-\\nbilities of a domain-specific model to further str',\n",
       " 'sets. Additionally, we utilize the advanced capa-\\nbilities of a domain-specific model to further strengthen the FLLM.\\n(1) Template QA Generation: This data comprises a list of\\nitems for which the model generates a binary response—assessing\\nwhether the items constitute a compatible outfit and sug-\\ngesting potential item candidates as suitable choices.\\n(2) LLM Auto QA Generation: The training prompts are crafted\\nto elicit detailed descriptions that highlight the style and fit\\nIntegrating Domain Kn',\n",
       " 's are crafted\\nto elicit detailed descriptions that highlight the style and fit\\nIntegrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations RecSys 2024, October 14–18, 2024, Bari, Italy\\nFigure 2: An overview of an information-augmented retrieval system featuring a fine-tuned Large Language Model (Fashion-\\nLLM) is presented. This system integrates multiple inputs into the Fashion-LLM to generate precise product descriptions.\\nFigure 3: Detailed Workflow of Model Fin',\n",
       " 'o the Fashion-LLM to generate precise product descriptions.\\nFigure 3: Detailed Workflow of Model Fine-Tuning and Re-\\ntrieval Processes for Fashion Recommender System. In this\\nAI-powered system, we employ both template-based AQ gen-\\neration and LLM auto-question generation to create domain-\\nrelevant questions and prompts.\\nof outfits. This enriches the model with comprehensive tex-\\ntual content, preserving and utilizing domain knowledge to\\nrefine its general understanding. Initially, the model gen',\n",
       " 'serving and utilizing domain knowledge to\\nrefine its general understanding. Initially, the model gener-\\nates multiple QAs based on fashion style, which are then\\nconverted into template training data. This also supports the\\ndomain knowledge needed for the retrieval module.\\nThese strategies, as reflected in the designed prompts, ensure\\nthat the FLLM not only retains its core language processing capabil-\\nities but is also finely tuned to analyze and address fashion-related\\nqueries with enhanced pre',\n",
       " 'bil-\\nities but is also finely tuned to analyze and address fashion-related\\nqueries with enhanced precision. Leveraging insights from domain\\nknowledge, including broad fashion trends and specific user pur-\\nchasing behaviors, the model crafts tailored training prompts. These\\nprompts are instrumental in boosting the FLLM’s predictive accu-\\nracy and its capacity to deliver customized fashion recommenda-\\ntions. By diversifying the retrieval process through these multi-path\\nqueries, the model signific',\n",
       " 'a-\\ntions. By diversifying the retrieval process through these multi-path\\nqueries, the model significantly broadens its ability to gather infor-\\nmation from various facets, culminating in a richly layered final\\ncontext. This multi-aspect query enhancement allows the model\\nnot only to retrieve more relevant information but also to provide\\nmore nuanced and comprehensive fashion item recommendations.\\n3.3 Retrieval augmented inference\\nWe introduce a novel architecture that incorporates a Retrieval-\\nA',\n",
       " '3.3 Retrieval augmented inference\\nWe introduce a novel architecture that incorporates a Retrieval-\\nAugmented Generation (RAG) model within a Large Language\\nModel (LLM) to enhance fashion item recommendations, as shown\\nin Figure 2. This architecture improves upon traditional retrieval\\nmethods by dividing the retrieval process into multiple query path-\\nways, each tailored to increase the contextual depth of the informa-\\ntion retrieved.\\nRecSys 2024, October 14–18, 2024, Bari, Italy Zhan Shi and Sha',\n",
       " 'depth of the informa-\\ntion retrieved.\\nRecSys 2024, October 14–18, 2024, Bari, Italy Zhan Shi and Shanglin Yang\\nThe architecture employs direct embedding-based queries, utiliz-\\ning the capabilities of neural embeddings for immediate and accu-\\nrate retrieval of fashion items. It also incorporates queries tailored\\nto style and occasion, enabling contextually aware recommenda-\\ntions that match specific user scenarios and stylistic preferences.\\nThe queries are aligned with the knowledge documented in',\n",
       " 'c user scenarios and stylistic preferences.\\nThe queries are aligned with the knowledge documented in our\\ngenerated knowledge path, as outlined in Figure 3. Additionally,\\nit leverages queries derived from LLM-generated questions. This\\ninnovative approach generates dynamic questions that reflect cur-\\nrent fashion trends and individual user profiles, further refining the\\nretrieval process.\\n4 EXPERIMENTS\\n4.1 Dataset\\nThe Polyvore dataset [5] is a robust, community-generated collec-\\ntion of fashion en',\n",
       " 'NTS\\n4.1 Dataset\\nThe Polyvore dataset [5] is a robust, community-generated collec-\\ntion of fashion ensembles, each meticulously curated to ensure that\\nthe individual items within them are complementary. It encom-\\npasses an extensive compilation of 68,000 manually curated outfits.\\nA specialized subset known as Polyvore-disjoint is derived from the\\nprimary Polyvore dataset. This subset is crafted by meticulously\\nfiltering out any outfits that share items across the training, val-\\nidation, and testi',\n",
       " 'meticulously\\nfiltering out any outfits that share items across the training, val-\\nidation, and testing sets, thereby ensuring a strict separation of\\ndata.\\n4.2 Evaluation\\nFITB accuracy. We carried out a fill-in-the-blank accuracy assess-\\nment using the Polyvore dataset, as detailed in Table 1. The results\\nindicate that, compared to traditional methods, the LLM demon-\\nstrates superior proficiency in grasping more complex contexts and\\nachieves the highest accuracy in both Joint and Disjoint Dataset',\n",
       " ' grasping more complex contexts and\\nachieves the highest accuracy in both Joint and Disjoint Dataset.\\nTable 1: Performance comparison of different methods on\\nPolyvore Outfits-D and Polyvore Outfits datasets.\\nMethod Polyvore Outfits-D Polyvore Outfits\\nType-Aware [18] 55.65 57.83\\nSCE-Net Average [22] 53.67 59.07\\nCSA-Net [8] 59.26 63.73\\nOutfitTransformer [15] 59.48 67.10\\nFashionLLM(ours) 62.17 67.21\\nFew-shot ability. We also evaluated the performance of our\\nmodel across different training data rati',\n",
       " 'Few-shot ability. We also evaluated the performance of our\\nmodel across different training data ratios to assess the impact of\\ndata volume on its effectiveness. A lower ratio indicates reduced\\ndata use for training. This evaluation compared our method to the\\nstate-of-the-art Type-Aware methods in fashion prediction. Our re-\\nsults show that although the final performance of both algorithms is\\nsimilar when using the full dataset, a significant difference emerges\\nat lower data ratios as shown in Fi',\n",
       " 'ar when using the full dataset, a significant difference emerges\\nat lower data ratios as shown in Figure 4. Here, our algorithm sur-\\npasses the baseline by about 10 percent in accuracy improvement.\\nThis demonstrates the Large Language Model’s (LLM) ability to\\neffectively learn from limited data and maintain high prediction\\naccuracy, which is especially advantageous in situations where\\ngathering extensive datasets is challenging, providing a significant\\nadvantage in data-sparse environments.\\ndata',\n",
       " 'tensive datasets is challenging, providing a significant\\nadvantage in data-sparse environments.\\ndata ratio\\n0%\\n20%\\n40%\\n60%\\n80%\\n10% 20% 30% 40% 50%\\nType_aware Ours\\nFIIB accuracy \\nFigure 4: Performance of different methods with respect to\\nFIIB at different of ratio of training data\\nFigure 5: Visualization of recommendation results: For each\\nquery item, our model can generate different outfits based\\non various style preferences.\\nVisualizing Recommendation Results. The visualization of\\nrecommendation',\n",
       " 'n various style preferences.\\nVisualizing Recommendation Results. The visualization of\\nrecommendation results, as depicted in Figure 5, demonstrates how\\nour approach can achieve diverse matching outcomes compatible\\nwith both the item and the style preferences. It effectively maps\\nquery items to style preferences and subsequently to corresponding\\nrecommendation items, showcasing the model’s adaptability and\\neffectiveness in aligning fashion choices with user preferences.\\n5 REMARKS AND FUTURE WORK\\n',\n",
       " 'lity and\\neffectiveness in aligning fashion choices with user preferences.\\n5 REMARKS AND FUTURE WORK\\nOur model has established a Large Language Model (LLM)-based\\nfashion recommendation system characterized by high accuracy\\nand dynamic adaptation. The LLM model can quickly adjust to user\\nIntegrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations RecSys 2024, October 14–18, 2024, Bari, Italy\\npreferences and new data distributions. Moving forward, our focus\\nwill be ',\n",
       " ' 14–18, 2024, Bari, Italy\\npreferences and new data distributions. Moving forward, our focus\\nwill be on a hybrid approach that combines the outputs of the LLM\\nand a Conventional Recommendation Model (CRM) to form a model\\nensemble. This ensemble will also incorporate multimodal informa-\\ntion, including images and metadata to further develop the system’s\\nability to process and integrate multimodal data, improving the\\naccuracy and personalization of fashion recommendations.\\nACKNOWLEDGMENTS\\nAcknowled',\n",
       " 'ta, improving the\\naccuracy and personalization of fashion recommendations.\\nACKNOWLEDGMENTS\\nAcknowledgements go here. Delete enclosing begin/end markers if\\nthere are no acknowledgements.\\nREFERENCES\\n[1] Ziad Al-Halah, Rainer Stiefelhagen, and Kristen Grauman. 2017. Fashion for-\\nward: Forecasting visual style in fashion. In Proceedings of the IEEE international\\nconference on computer vision . 388–397.\\n[2] Victor Carbune, Hassan Mansoor, Fangyu Liu, Rahul Aralikatte, Gilles Baechler,\\nJindong Chen, a',\n",
       " '.\\n[2] Victor Carbune, Hassan Mansoor, Fangyu Liu, Rahul Aralikatte, Gilles Baechler,\\nJindong Chen, and Abhanshu Sharma. 2024. Chart-based reasoning: Transferring\\ncapabilities from llms to vlms. arXiv preprint arXiv:2403.12596 (2024).\\n[3] Wen-Huang Cheng, Sijie Song, Chieh-Yun Chen, Shintami Chusnul Hidayati,\\nand Jiaying Liu. 2021. Fashion Meets Computer Vision: A Survey. ACM Comput.\\nSurv. 54, 4, Article 72 (jul 2021), 41 pages. https://doi.org/10.1145/3447239\\n[4] Yujuan Ding, Zhihui Lai, PY Mok,',\n",
       " 'rticle 72 (jul 2021), 41 pages. https://doi.org/10.1145/3447239\\n[4] Yujuan Ding, Zhihui Lai, PY Mok, and Tat-Seng Chua. 2023. Computational\\nTechnologies for Fashion Recommendation: A Survey. Comput. Surveys 56, 5\\n(2023), 1–45.\\n[5] Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S Davis. 2017. Learning\\nfashion compatibility with bidirectional lstms. In Proceedings of the 25th ACM\\ninternational conference on Multimedia . 1078–1086.\\n[6] Hyunwoo Hwangbo, Yang Sok Kim, and Kyung Jin Cha. 2018. Recom',\n",
       " 'ference on Multimedia . 1078–1086.\\n[6] Hyunwoo Hwangbo, Yang Sok Kim, and Kyung Jin Cha. 2018. Recommendation\\nsystem development for fashion retail e-commerce.Electronic Commerce Research\\nand Applications 28 (2018), 94–101.\\n[7] Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan\\nWang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang.\\n2023. CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evalua-\\ntion of Large Language Model Generation. ar',\n",
       " 'ling LLM-as-Critic for Effective and Explainable Evalua-\\ntion of Large Language Model Generation. arXiv:2311.18702 [cs.CL]\\n[8] Yen-Liang Lin, Son Tran, and Larry S. Davis. 2020. Fashion Outfit Complementary\\nItem Retrieval. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR). IEEE. https://doi.org/10.1109/cvpr42600.2020.00337\\n[9] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boosting\\ncontent-based recommendation with both open-and closed-source larg',\n",
       " 'iao-Ming Wu. 2024. Once: Boosting\\ncontent-based recommendation with both open-and closed-source large language\\nmodels. In Proceedings of the 17th ACM International Conference on Web Search\\nand Data Mining. 452–461.\\n[10] Xin Liu, Yongbin Sun, Ziwei Liu, and Dahua Lin. 2020. Learning diverse fashion\\ncollocation by neural graph filtering. IEEE Transactions on Multimedia 23 (2020),\\n2894–2901.\\n[11] Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. 2023. Llm-\\nrec: Personalized recomme',\n",
       " 'jia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. 2023. Llm-\\nrec: Personalized recommendation via prompting large language models. arXiv\\npreprint arXiv:2307.15780 (2023).\\n[12] Dongmei Mo, Xingxing Zou, Kaicheng Pang, and Wai Keung Wong. 2023. To-\\nwards private stylists via personalized compatibility learning. Expert Systems\\nwith Applications 219 (2023), 119632.\\n[13] Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and\\nIoannis Kompatsiaris. 2022. Multimodal Qua',\n",
       " ' Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and\\nIoannis Kompatsiaris. 2022. Multimodal Quasi-AutoRegression: Forecasting the\\nvisual popularity of new fashion products. International Journal of Multimedia\\nInformation Retrieval 11, 4 (2022), 717–729.\\n[14] Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and\\nIoannis Kompatsiaris. 2022. VICTOR: Visual Incompatibility Detection with\\nTransformers and Fashion-specific contrastive pre-training. arXiv preprint\\narXiv:2207.1',\n",
       " 'tection with\\nTransformers and Fashion-specific contrastive pre-training. arXiv preprint\\narXiv:2207.13458 (2022).\\n[15] Rohan Sarkar, Navaneeth Bodla, Mariya I Vasileva, Yen-Liang Lin, Anurag Beni-\\nwal, Alan Lu, and Gerard Medioni. 2023. Outfittransformer: Learning outfit\\nrepresentations for fashion recommendation. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vision . 3601–3609.\\n[16] Maria Anastassia Stefani, Vassilios Stefanis, and John Garofalakis. 2019. CFRS: a\\nt',\n",
       " ' 3601–3609.\\n[16] Maria Anastassia Stefani, Vassilios Stefanis, and John Garofalakis. 2019. CFRS: a\\ntrends-driven collaborative fashion recommendation system. In 2019 10th Inter-\\nnational Conference on Information, Intelligence, Systems and Applications (IISA) .\\nIEEE, 1–4.\\n[17] Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha\\nKumar, and David Forsyth. 2018. Learning Type-Aware Embeddings for Fashion\\nCompatibility. In Proceedings of the European Conference on Computer V',\n",
       " '-Aware Embeddings for Fashion\\nCompatibility. In Proceedings of the European Conference on Computer Vision\\n(ECCV).\\n[18] Mariya I Vasileva, Bryan A Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha\\nKumar, and David Forsyth. 2018. Learning type-aware embeddings for fashion\\ncompatibility. InProceedings of the European conference on computer vision (ECCV).\\n390–405.\\n[19] Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge\\nBelongie. 2015. Learning visual clothing style with het',\n",
       " 'Bell, Julian McAuley, Kavita Bala, and Serge\\nBelongie. 2015. Learning visual clothing style with heterogeneous dyadic co-\\noccurrences. In Proceedings of the IEEE international conference on computer vision.\\n4642–4650.\\n[20] Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and\\nYang Tang. 2023. A brief overview of ChatGPT: The history, status quo and\\npotential future development. IEEE/CAA Journal of Automatica Sinica 10, 5\\n(2023), 1122–1136.\\n[21] Ruobing Xie, Yanlei Liu, Shaol',\n",
       " '. IEEE/CAA Journal of Automatica Sinica 10, 5\\n(2023), 1122–1136.\\n[21] Ruobing Xie, Yanlei Liu, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin.\\n2021. Personalized approximate pareto-efficient recommendation. In Proceedings\\nof the Web Conference 2021. 3839–3849.\\n[22] Siyuan Xing, Qiulei Dong, and Zhanyi Hu. 2022. SCE-Net: Self-and cross-\\nenhancement network for single-view height estimation and semantic segmenta-\\ntion. Remote Sensing 14, 9 (2022), 2252.\\n[23] Fan Yang, Zheng Chen, Ziyan Jiang, E',\n",
       " 'emantic segmenta-\\ntion. Remote Sensing 14, 9 (2022), 2252.\\n[23] Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin\\nLu. 2023. Palr: Personalization aware llms for recommendation. arXiv preprint\\narXiv:2305.07622 (2023).\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca06bf",
   "metadata": {},
   "source": [
    "## Create Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005b2c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text_model = AutoModel.from_pretrained(model_name)\n",
    "text_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "806cd1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "# tes embedding\n",
    "\n",
    "text = \"This is a test sentence\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = text_model(**inputs)\n",
    "\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89df9f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def embed_texts(texts):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = text_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# call the function\n",
    "pdf_embeddings = embed_texts(pdf_chunks)\n",
    "pdf_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2723860",
   "metadata": {},
   "source": [
    "## Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e987602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dim = pdf_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim) # Cosine similarity\n",
    "faiss.normalize_L2(pdf_embeddings)\n",
    "\n",
    "index.add(pdf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595f4e9",
   "metadata": {},
   "source": [
    "## Query FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86bf785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, k=5):\n",
    "    q_emb = embed_texts([query])\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, idxs = index.search(q_emb, k)\n",
    "    return [pdf_chunks[i] for i in idxs[0]]\n",
    "\n",
    "\"\"\"Create a function to query FAISS index and retrieve relevant contexts based on PDF chunks.\"\"\"\n",
    "contexts = search_faiss(\"What outfits are suitable for a summer wear?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6917420",
   "metadata": {},
   "source": [
    "## RAG with LLM (Context-Aware Generation)\n",
    "---\n",
    "## Purpose\n",
    "* ### Use retrieved context\n",
    "* ### Prevent hallucination\n",
    "* ### Generate grounded answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "542c6e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c1048fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69eb01c",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1397d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create base_dir without __file__\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"06_Finetuning_LLM_RAG.ipynb\"))\n",
    "load_dotenv(os.path.join(BASE_DIR, \".env\"))\n",
    "\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "client = InferenceClient(\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4818ad3",
   "metadata": {},
   "source": [
    "## Build RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76398ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(contexts, question):\n",
    "    context_block = \"\\n\\n\".join(contexts)\n",
    "    return f\"\"\"You are a fashion recommendation assistant expert.\n",
    "\n",
    "Context:\n",
    "{context_block}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f6f39",
   "metadata": {},
   "source": [
    "## Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30fcb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided references, I'll provide a fashion recommendation for a summer casual outfit suitable for casual occasions.\n",
      "\n",
      "Considering the limitations of traditional methods, which are constrained to aligning with the training data and struggle to adapt to new preference distributions, I'll suggest a solution that leverages Large Language Models (LLMs) for improved fashion recommendations.\n",
      "\n",
      "For a summer casual outfit, I recommend the following combination of items:\n",
      "\n",
      "1. **Top:** A lightweight, pastel-colored blouse with a relaxed fit and V-neckline. This style is perfect for warm weather and can be easily paired with shorts or a flowy skirt.\n",
      "2. **Bottom:** Distressed denim shorts in a light wash. These are comfortable, versatile, and can be dressed up or down depending on the occasion.\n",
      "3. **Shoes:** A pair of white sneakers with a breathable mesh upper. This style is perfect for casual outings and can add a sporty touch to the overall look.\n",
      "4. **Accessories:** A pair of\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(question):\n",
    "    contexts = search_faiss(question)\n",
    "    prompt = build_prompt(contexts, question)\n",
    "    \n",
    "    # Use chat_completion with messages format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a fashion recommendation assistant expert.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat_completion(\n",
    "        messages=messages,\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(generate_answer(\"Recommend a summer casual outfit for casual occasions.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9f7bd",
   "metadata": {},
   "source": [
    "# Re-Ranking Model\n",
    "---\n",
    "## Purpose\n",
    "* ### Improves quality\n",
    "* ### Applies business logic\n",
    "* ### Applies business logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c5af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Model: 384 (query) + 384 (item) + 5 (numeric) = 773\n",
    "rerank_input = layers.Input(shape=(384 + 384 + 5,))  # Changed from 128+5 to 773\n",
    "x = layers.Dense(128, activation='relu')(rerank_input)  # Increased from 64\n",
    "x = layers.Dense(64, activation='relu')(x)              # Increased from 32\n",
    "score = layers.Dense(1, activation='sigmoid')(x)\n",
    "rerank_model = tf.keras.Model(inputs=rerank_input, outputs=score)\n",
    "\n",
    "rerank_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ec5b6",
   "metadata": {},
   "source": [
    "# Re-Ranking Works in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2a88bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 15:06:05.515220: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6760\n",
      "Score: 0.6626\n",
      "Score: 0.6442\n",
      "Score: 0.6305\n",
      "Score: 0.6299\n"
     ]
    }
   ],
   "source": [
    "def rerank(candidates, query_embedding):\n",
    "    scores = []\n",
    "    for item in candidates:\n",
    "        combined = np.concatenate([\n",
    "            query_embedding,           # (384,)\n",
    "            item['embedding'],         # (384,)\n",
    "            item['numeric_features']   # (5,)\n",
    "        ])  # (773,)\n",
    "        \n",
    "        score = rerank_model.predict(combined.reshape(1, -1), verbose=0)[0]\n",
    "        scores.append(score)\n",
    "    \n",
    "    return sorted(\n",
    "        zip(candidates, scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "# Example with 384-dim embeddings\n",
    "candidates = [\n",
    "    {\n",
    "        'embedding': np.random.rand(384),  # Changed from 128\n",
    "        'numeric_features': np.random.rand(5)\n",
    "    } for _ in range(10)\n",
    "]\n",
    "query_embedding = np.random.rand(384)  # Changed from 128\n",
    "\n",
    "ranked_results = rerank(candidates, query_embedding=query_embedding)\n",
    "for item, score in ranked_results[:5]:\n",
    "    print(f\"Score: {score[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0691cc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Check what dimension your FAISS index uses\n",
    "print(f\"FAISS index dimension: {index.d}\")\n",
    "\n",
    "# Then use that dimension\n",
    "embedding_dim = index.d\n",
    "candidates = [\n",
    "    {\n",
    "        'embedding': np.random.rand(embedding_dim),\n",
    "        'numeric_features': np.random.rand(5)\n",
    "    } for _ in range(10)\n",
    "]\n",
    "query_embedding = np.random.rand(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a585c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index dimension: 384\n",
      "Embedding dimension used: 384\n",
      "Rerank model input shape: (None, 773)\n"
     ]
    }
   ],
   "source": [
    "# Check Faiss, embedding dim, and rerank model what we have used\n",
    "print(f\"FAISS index dimension: {index.d}\")\n",
    "print(f\"Embedding dimension used: {embedding_dim}\")\n",
    "print(f\"Rerank model input shape: {rerank_model.input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa598f",
   "metadata": {},
   "source": [
    "## * Save FAISS index + chunks\n",
    "## *  Save embedding model (Hugging Face)\n",
    "## * Save re-ranker model (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e17fa537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Save FAISS index + chunks\n",
    "os.makedirs(\"artifacts/faiss\", exist_ok=True)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"artifacts/faiss/index.faiss\")\n",
    "\n",
    "# Save chunks (important!)\n",
    "with open(\"artifacts/faiss/chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pdf_chunks, f)\n",
    "\n",
    "# Save embedding model (Hugging Face)\n",
    "os.makedirs(\"artifacts/embedding_model\", exist_ok=True)\n",
    "\n",
    "tokenizer.save_pretrained(\"artifacts/embedding_model/all-MiniLM-L6-v2\")\n",
    "text_model.save_pretrained(\"artifacts/embedding_model/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Save re-ranker model (Keras)\n",
    "os.makedirs(\"artifacts/reranker\", exist_ok=True)\n",
    "\n",
    "rerank_model.save(\"artifacts/reranker/model.keras\")\n",
    "\n",
    "# Optional but recommended: save feature schema\n",
    "import json\n",
    "config = {\n",
    "    \"query_embedding_dim\": 384,\n",
    "    \"doc_embedding_dim\": 384,\n",
    "    \"numeric_features_dim\": 5,\n",
    "    \"total_features\": 773\n",
    "}\n",
    "\n",
    "with open(\"artifacts/reranker/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d51622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "index = faiss.read_index(\"../models/RAG_FAISS_LLM/faiss/index.faiss\")\n",
    "\n",
    "with open(\"../models/RAG_FAISS_LLM/faiss/chunks.pkl\", \"rb\") as f:\n",
    "    pdf_chunks = pickle.load(f)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"../models/RAG_FAISS_LLM/embedding_model/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "text_model = AutoModel.from_pretrained(\n",
    "    \"../models/RAG_FAISS_LLM/embedding_model/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "text_model.eval()\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "rerank_model = tf.keras.models.load_model(\n",
    "    \"../models/RAG_FAISS_LLM/reranker/model.keras\"\n",
    ")\n",
    "\n",
    "with open(\"../models/RAG_FAISS_LLM/reranker/config.json\") as f:\n",
    "    rerank_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a58f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
