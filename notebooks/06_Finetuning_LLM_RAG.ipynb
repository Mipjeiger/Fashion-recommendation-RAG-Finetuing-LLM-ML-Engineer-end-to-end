{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45690c80",
   "metadata": {},
   "source": [
    "## RAG Finetuning LLM\n",
    "### * **Retrieval based on PDF relevant about fashion recommendation to Vector Database**\n",
    "### * **Integrating LLM-based attribute aware context with fine-grained fashion retrieval. For each attribute in the query the LLM first generates a detailed attribute-aware context for enriching attribute representations with commonsense business insight requirements.**\n",
    "### * **The attribute embeddings, enriched with their attribute- aware context, form a conditional query vector that guides the retrieval process, interacting with image patches to focus on relevant regions that match the specified attributes.**\n",
    "### * **Prompt generation training strategies to enhance its capacity for delivering personalized fashion advice while retaining essential domain knowledge.**\n",
    "### * **Generative images AI Engineering.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce9a81",
   "metadata": {},
   "source": [
    "# LLM Strategies\n",
    "### These strategies, as reflected in the designed prompts, \n",
    "### Ensure that the LLM not only retains its core language processing capabilities but is also finely tuned to analyze and address fashion-related queries with enhanced precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c202f",
   "metadata": {},
   "source": [
    "## Load & chunk PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127674f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# call the functions\n",
    "pdf_text = load_pdf_text(\"../data/pdf/fashion recommendation LLM.pdf\")\n",
    "pdf_chunks = chunk_text(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2401b736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Integrating Domain Knowledge into Large Language Models for\\nEnhanced Fashion Recommendations\\nZhan Shi∗∗\\naria2@scu.edu\\nSanta Clara University\\nSanta Clara, USA\\nShanglin Yang†\\nkudoysl@gmail.com\\nABSTRACT\\nFashion, deeply rooted in sociocultural dynamics, evolves as individ-\\nuals emulate styles popularized by influencers and iconic figures. In\\nthe quest to replicate such refined tastes using artificial intelligence,\\ntraditional fashion ensemble methods have primarily used super-\\nvised learning to imit',\n",
       " 'intelligence,\\ntraditional fashion ensemble methods have primarily used super-\\nvised learning to imitate the decisions of style icons, which falter\\nwhen faced with distribution shifts, leading to style replication dis-\\ncrepancies triggered by slight variations in input. Meanwhile, large\\nlanguage models (LLMs) have become prominent across various\\nsectors, recognized for their user-friendly interfaces, strong con-\\nversational skills, and advanced reasoning capabilities. To address\\nthese challenges,',\n",
       " 's, strong con-\\nversational skills, and advanced reasoning capabilities. To address\\nthese challenges, we introduce the Fashion Large Language Model\\n(FLLM), which employs auto-prompt generation training strate-\\ngies to enhance its capacity for delivering personalized fashion\\nadvice while retaining essential domain knowledge. Additionally,\\nby integrating a retrieval augmentation technique during inference,\\nthe model can better adjust to individual preferences. Our results\\nshow that this approach su',\n",
       " 'rence,\\nthe model can better adjust to individual preferences. Our results\\nshow that this approach surpasses existing models in accuracy,\\ninterpretability, and few-shot learning capabilities.\\nCCS CONCEPTS\\n•Information systems → Novelty in information retrieval.\\nKEYWORDS\\nFashion Recommendation, LLM, RAG\\nACM Reference Format:\\nZhan Shi and Shanglin Yang. 2024. Integrating Domain Knowledge into\\nLarge Language Models for Enhanced Fashion Recommendations. In Pro-\\nceedings of Make sure to enter the corr',\n",
       " 'anguage Models for Enhanced Fashion Recommendations. In Pro-\\nceedings of Make sure to enter the correct conference title from your rights\\nconfirmation email (RecSys 2024). ACM, New York, NY, USA, 5 pages. https:\\n//doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nFashion plays a significant role in various aspects of life, including\\nsocial and cultural identity. People often imitate styles suggested\\nby fashion experts, icons, or Key Opinion Leaders (KOLs) from\\n∗Both authors contributed equally to this rese',\n",
       " 'on experts, icons, or Key Opinion Leaders (KOLs) from\\n∗Both authors contributed equally to this research.\\n†Both authors contributed equally to this research.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than t',\n",
       " 'd the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nRecSys 2024, October 14–18, 2024, Bari, Italy\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/1',\n",
       " 'yright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nFigure 1: Fashion item recommendations can differ signif-\\nicantly based on various occasions, styles, and attributes.\\n(a) Traditional methods (red box), trained solely on fixed\\ndatasets using sequence models, are constrained to aligning\\nwith the training data and struggle to adapt to new prefer-\\nence distributions. In contrast, our (b) LLM methods (',\n",
       " 'ing data and struggle to adapt to new prefer-\\nence distributions. In contrast, our (b) LLM methods (green\\nbox) dynamically integrate diverse contexts into the prompt.\\nLeveraging the inference capabilities of large models, our\\nmethods produce results under a wide range of conditions.\\nsocial media. In the rapidly evolving fashion technology sector, cre-\\nating advanced, personalized recommendation systems is a major\\nacademic and commercial endeavor. This research introduces an\\ninnovative approach u',\n",
       " 'stems is a major\\nacademic and commercial endeavor. This research introduces an\\ninnovative approach using Large Language Models (LLMs) to trans-\\nform fashion recommendations. Unlike traditional methods relying\\non static datasets and supervised learning, our generative model\\nleverages LLMs to provide personalized, flexible fashion advice.\\nThis approach overcomes previous limitations and offers a deeper\\nconnection with individual preferences, reflecting the dynamic na-\\nture of fashion. Fashion tren',\n",
       " 'per\\nconnection with individual preferences, reflecting the dynamic na-\\nture of fashion. Fashion trends change rapidly, posing challenges\\nfor recommendation systems to forecast and adapt to personal pref-\\nerences. Conventional systems, though somewhat successful in\\npredicting item compatibility and ensuring stylistic consistency,\\noften fail to cater to diverse individual styles and rely heavily on\\nstatic datasets, limiting their relevance over time.\\nTo address these challenges, our study presents',\n",
       " 'static datasets, limiting their relevance over time.\\nTo address these challenges, our study presents an innovative so-\\nlution using LLMs’ generative and adaptive strengths [20], offering\\nnew methods for generating recommendations that include items,\\noccasions, and styles. We propose a new training paradigm for\\narXiv:2502.15696v1  [cs.CL]  3 Jan 2025\\nRecSys 2024, October 14–18, 2024, Bari, Italy Zhan Shi and Shanglin Yang\\nLLMs, starting with fashion-specific data to retain core knowledge\\nand prog',\n",
       " 'an Shi and Shanglin Yang\\nLLMs, starting with fashion-specific data to retain core knowledge\\nand progressing through specialized training phases. This includes\\na foundational phase with ’FIIB’ data, a combined phase of style\\nand ’FIIB’ data, and the development of a custom dataset for final\\nrefinement.\\nThe paper is structured as follows: We first explore the current\\nlandscape of fashion recommendation systems, highlighting their\\nlimitations and the potential of LLMs. We then detail our novel\\nappr',\n",
       " 'ion systems, highlighting their\\nlimitations and the potential of LLMs. We then detail our novel\\napproach, including the architecture of our LLM-based system, data\\nsources, training procedure, and the use of retrieval-augmented\\ngeneration. Following this, we describe our experimental design,\\nevaluation metrics, and empirical study results, demonstrating our\\nmodel’s superior performance in accuracy, adaptability, and user\\nsatisfaction. We conclude with a discussion of the broader implica-\\ntions an',\n",
       " ' adaptability, and user\\nsatisfaction. We conclude with a discussion of the broader implica-\\ntions and future research directions. Our key contributions are:\\n• Reconceptualizing fashion recommendation as a generative\\nchallenge using LLMs.\\n• Incorporating domain fine-tuning and retrieval-augmented\\ngeneration to enable the model to learn new fashion knowl-\\nedge while preserving domain expertise.\\n• Demonstrating the model’s performance through experi-\\nments, highlighting improved accuracy, impressiv',\n",
       " 'monstrating the model’s performance through experi-\\nments, highlighting improved accuracy, impressive few-shot\\nlearning capabilities, and adaptability to different contexts.\\n2 RELATED WORK\\nIn recent years, the use of deep learning neural networks, high-\\nlighted by [ 3], has drawn significant attention in various tasks\\nwithin the fashion industry. These tasks include category and at-\\ntribute classification ([13]), trend forecasting ([1], popularity pre-\\ndiction ([14]), and the development of fash',\n",
       " 'fication ([13]), trend forecasting ([1], popularity pre-\\ndiction ([14]), and the development of fashion recommendation\\nsystems ([6], [16]), particularly in outfit recommendation. It’s cru-\\ncial to determine which garments go well together to recommend\\ncomplete outfits that are compatible and cohesive.\\nInitial attempts to address outfit compatibility treated it as a\\nseries of pairwise comparisons among all garments in an outfit,\\nas explored by [17]. These pairwise-based methods used Siamese\\nnetwo',\n",
       " 'mong all garments in an outfit,\\nas explored by [17]. These pairwise-based methods used Siamese\\nnetworks ([19]) and triplet loss networks, with either type-aware\\nembeddings ([18]) or similarity-aware embeddings ([18]). In con-\\ntrast, some researchers have aimed to capture a holistic view of\\noutfit-level representations through bidirectional LSTMs ([5]) or\\ngraph neural networks ([4], [10]). Some studies also have shifted to\\nattention-based methods, including the Transformer architecture\\nfor person',\n",
       " 'dies also have shifted to\\nattention-based methods, including the Transformer architecture\\nfor personalized outfit recommendations and complementary item\\nretrieval ([12], [21], [15]).\\nResearchers have increasingly turned to Large Language Models\\n(LLMs) for their outstanding text generation capabilities, particu-\\nlarly for data augmentation ([7], [9]). [2], for instance, used LLMs\\nto create multimodal datasets that blend language and images for\\ninstruction-following tasks. Their method, which invo',\n",
       " 'al datasets that blend language and images for\\ninstruction-following tasks. Their method, which involved tuning\\nthe instructions based on this data, significantly improved both\\nvision and language comprehension.\\nIn the domain of personalized recommendation systems, LLMs\\nhave also played a crucial role. [ 23] employed LLMs to generate\\nuser profiles by incorporating behaviors like clicks, purchases, and\\nratings. These profiles, when combined with the history of user\\ninteractions and potential item',\n",
       " ' and\\nratings. These profiles, when combined with the history of user\\ninteractions and potential items, were key in formulating the final\\nrecommendation prompt. LLMs were then applied to determine the\\nlikelihood of user-item interactions from this prompt. In a similar\\nvein, [11] developed a technique that leverages LLMs to merge\\nreasoning about user preferences with factual knowledge of items.\\nHowever, our research explores a new area: there is no established\\nwork on using LLMs in fashion recomme',\n",
       " 'ver, our research explores a new area: there is no established\\nwork on using LLMs in fashion recommendation. Therefore, our\\nstudy establishes a novel baseline for LLM application in the fashion\\nrecommendation field.\\n3 METHODOLOGY\\n3.1 Problem Formulation\\nFigure 2 illustrates our sophisticated fashion recommendation sys-\\ntem, which utilizes a finely-tuned Large Language Model (LLM)\\nintegrated with domain-specific retrieval processes. This system,\\noptimized with a dedicated fashion dataset, is adep',\n",
       " 'omain-specific retrieval processes. This system,\\noptimized with a dedicated fashion dataset, is adept at parsing the\\ncomplexities of fashion nuances. It employs the LLM to create spe-\\ncific prompts that trigger searches within a comprehensive fashion\\nproduct dataset, and a dual-component mechanism retrieves vital\\ndomain context by merging industry trends and design principles\\nwith user preferences to tailor recommendations.\\nThe system processes both user and system-generated text via\\ntokenizers,',\n",
       " ' to tailor recommendations.\\nThe system processes both user and system-generated text via\\ntokenizers, converting them into tokens suitable for the LLM. The\\nmodel then analyzes these inputs to generate customized recom-\\nmendations, such as suggesting \"floral-print pants\" among various\\nitem choices. Fashion-related documents are processed into em-\\nbeddings through a sentence transformer and stored in a vector\\ndatabase. These embeddings are then matched with user query\\nembeddings to further refine t',\n",
       " ' a vector\\ndatabase. These embeddings are then matched with user query\\nembeddings to further refine the recommendations.\\nUltimately, the LLM delivers sophisticated responses to user\\nqueries, resulting in either well-coordinated outfit recommenda-\\ntions or detailed fashion advice. This ensures both contextual rel-\\nevance and personalization. By integrating system prompts, item\\ndescription and processed contextual data, the system achieves\\nprecise and personalized fashion recommendations.\\n3.2 Fintu',\n",
       " 'sed contextual data, the system achieves\\nprecise and personalized fashion recommendations.\\n3.2 Fintuning LLM with auto-prompts\\ngeneration\\nThe diagram illustrated in Figure 3 delineates the comprehensive\\ntraining and refinement process for a specialized Fashion Large\\nLanguage Model (FLLM), which is tasked with dispensing tailored\\nfashion advice. Initially, the FLLM is acquainted with Polyvore out-\\nfit training data. This data encompasses two primary types: binary\\nquestion data, which prompts the ',\n",
       " 'fit training data. This data encompasses two primary types: binary\\nquestion data, which prompts the model to discern the appropri-\\nateness of an outfit, and Fill-in-the-Blank (FITB) data.\\nTo enhance the robustness and proficiency of our Fashion Large\\nLanguage Model (FLLM), we employ a dual-strategy approach in\\ngenerating training data. Initially, we source training data directly\\nfrom existing datasets. Additionally, we utilize the advanced capa-\\nbilities of a domain-specific model to further str',\n",
       " 'sets. Additionally, we utilize the advanced capa-\\nbilities of a domain-specific model to further strengthen the FLLM.\\n(1) Template QA Generation: This data comprises a list of\\nitems for which the model generates a binary response—assessing\\nwhether the items constitute a compatible outfit and sug-\\ngesting potential item candidates as suitable choices.\\n(2) LLM Auto QA Generation: The training prompts are crafted\\nto elicit detailed descriptions that highlight the style and fit\\nIntegrating Domain Kn',\n",
       " 's are crafted\\nto elicit detailed descriptions that highlight the style and fit\\nIntegrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations RecSys 2024, October 14–18, 2024, Bari, Italy\\nFigure 2: An overview of an information-augmented retrieval system featuring a fine-tuned Large Language Model (Fashion-\\nLLM) is presented. This system integrates multiple inputs into the Fashion-LLM to generate precise product descriptions.\\nFigure 3: Detailed Workflow of Model Fin',\n",
       " 'o the Fashion-LLM to generate precise product descriptions.\\nFigure 3: Detailed Workflow of Model Fine-Tuning and Re-\\ntrieval Processes for Fashion Recommender System. In this\\nAI-powered system, we employ both template-based AQ gen-\\neration and LLM auto-question generation to create domain-\\nrelevant questions and prompts.\\nof outfits. This enriches the model with comprehensive tex-\\ntual content, preserving and utilizing domain knowledge to\\nrefine its general understanding. Initially, the model gen',\n",
       " 'serving and utilizing domain knowledge to\\nrefine its general understanding. Initially, the model gener-\\nates multiple QAs based on fashion style, which are then\\nconverted into template training data. This also supports the\\ndomain knowledge needed for the retrieval module.\\nThese strategies, as reflected in the designed prompts, ensure\\nthat the FLLM not only retains its core language processing capabil-\\nities but is also finely tuned to analyze and address fashion-related\\nqueries with enhanced pre',\n",
       " 'bil-\\nities but is also finely tuned to analyze and address fashion-related\\nqueries with enhanced precision. Leveraging insights from domain\\nknowledge, including broad fashion trends and specific user pur-\\nchasing behaviors, the model crafts tailored training prompts. These\\nprompts are instrumental in boosting the FLLM’s predictive accu-\\nracy and its capacity to deliver customized fashion recommenda-\\ntions. By diversifying the retrieval process through these multi-path\\nqueries, the model signific',\n",
       " 'a-\\ntions. By diversifying the retrieval process through these multi-path\\nqueries, the model significantly broadens its ability to gather infor-\\nmation from various facets, culminating in a richly layered final\\ncontext. This multi-aspect query enhancement allows the model\\nnot only to retrieve more relevant information but also to provide\\nmore nuanced and comprehensive fashion item recommendations.\\n3.3 Retrieval augmented inference\\nWe introduce a novel architecture that incorporates a Retrieval-\\nA',\n",
       " '3.3 Retrieval augmented inference\\nWe introduce a novel architecture that incorporates a Retrieval-\\nAugmented Generation (RAG) model within a Large Language\\nModel (LLM) to enhance fashion item recommendations, as shown\\nin Figure 2. This architecture improves upon traditional retrieval\\nmethods by dividing the retrieval process into multiple query path-\\nways, each tailored to increase the contextual depth of the informa-\\ntion retrieved.\\nRecSys 2024, October 14–18, 2024, Bari, Italy Zhan Shi and Sha',\n",
       " 'depth of the informa-\\ntion retrieved.\\nRecSys 2024, October 14–18, 2024, Bari, Italy Zhan Shi and Shanglin Yang\\nThe architecture employs direct embedding-based queries, utiliz-\\ning the capabilities of neural embeddings for immediate and accu-\\nrate retrieval of fashion items. It also incorporates queries tailored\\nto style and occasion, enabling contextually aware recommenda-\\ntions that match specific user scenarios and stylistic preferences.\\nThe queries are aligned with the knowledge documented in',\n",
       " 'c user scenarios and stylistic preferences.\\nThe queries are aligned with the knowledge documented in our\\ngenerated knowledge path, as outlined in Figure 3. Additionally,\\nit leverages queries derived from LLM-generated questions. This\\ninnovative approach generates dynamic questions that reflect cur-\\nrent fashion trends and individual user profiles, further refining the\\nretrieval process.\\n4 EXPERIMENTS\\n4.1 Dataset\\nThe Polyvore dataset [5] is a robust, community-generated collec-\\ntion of fashion en',\n",
       " 'NTS\\n4.1 Dataset\\nThe Polyvore dataset [5] is a robust, community-generated collec-\\ntion of fashion ensembles, each meticulously curated to ensure that\\nthe individual items within them are complementary. It encom-\\npasses an extensive compilation of 68,000 manually curated outfits.\\nA specialized subset known as Polyvore-disjoint is derived from the\\nprimary Polyvore dataset. This subset is crafted by meticulously\\nfiltering out any outfits that share items across the training, val-\\nidation, and testi',\n",
       " 'meticulously\\nfiltering out any outfits that share items across the training, val-\\nidation, and testing sets, thereby ensuring a strict separation of\\ndata.\\n4.2 Evaluation\\nFITB accuracy. We carried out a fill-in-the-blank accuracy assess-\\nment using the Polyvore dataset, as detailed in Table 1. The results\\nindicate that, compared to traditional methods, the LLM demon-\\nstrates superior proficiency in grasping more complex contexts and\\nachieves the highest accuracy in both Joint and Disjoint Dataset',\n",
       " ' grasping more complex contexts and\\nachieves the highest accuracy in both Joint and Disjoint Dataset.\\nTable 1: Performance comparison of different methods on\\nPolyvore Outfits-D and Polyvore Outfits datasets.\\nMethod Polyvore Outfits-D Polyvore Outfits\\nType-Aware [18] 55.65 57.83\\nSCE-Net Average [22] 53.67 59.07\\nCSA-Net [8] 59.26 63.73\\nOutfitTransformer [15] 59.48 67.10\\nFashionLLM(ours) 62.17 67.21\\nFew-shot ability. We also evaluated the performance of our\\nmodel across different training data rati',\n",
       " 'Few-shot ability. We also evaluated the performance of our\\nmodel across different training data ratios to assess the impact of\\ndata volume on its effectiveness. A lower ratio indicates reduced\\ndata use for training. This evaluation compared our method to the\\nstate-of-the-art Type-Aware methods in fashion prediction. Our re-\\nsults show that although the final performance of both algorithms is\\nsimilar when using the full dataset, a significant difference emerges\\nat lower data ratios as shown in Fi',\n",
       " 'ar when using the full dataset, a significant difference emerges\\nat lower data ratios as shown in Figure 4. Here, our algorithm sur-\\npasses the baseline by about 10 percent in accuracy improvement.\\nThis demonstrates the Large Language Model’s (LLM) ability to\\neffectively learn from limited data and maintain high prediction\\naccuracy, which is especially advantageous in situations where\\ngathering extensive datasets is challenging, providing a significant\\nadvantage in data-sparse environments.\\ndata',\n",
       " 'tensive datasets is challenging, providing a significant\\nadvantage in data-sparse environments.\\ndata ratio\\n0%\\n20%\\n40%\\n60%\\n80%\\n10% 20% 30% 40% 50%\\nType_aware Ours\\nFIIB accuracy \\nFigure 4: Performance of different methods with respect to\\nFIIB at different of ratio of training data\\nFigure 5: Visualization of recommendation results: For each\\nquery item, our model can generate different outfits based\\non various style preferences.\\nVisualizing Recommendation Results. The visualization of\\nrecommendation',\n",
       " 'n various style preferences.\\nVisualizing Recommendation Results. The visualization of\\nrecommendation results, as depicted in Figure 5, demonstrates how\\nour approach can achieve diverse matching outcomes compatible\\nwith both the item and the style preferences. It effectively maps\\nquery items to style preferences and subsequently to corresponding\\nrecommendation items, showcasing the model’s adaptability and\\neffectiveness in aligning fashion choices with user preferences.\\n5 REMARKS AND FUTURE WORK\\n',\n",
       " 'lity and\\neffectiveness in aligning fashion choices with user preferences.\\n5 REMARKS AND FUTURE WORK\\nOur model has established a Large Language Model (LLM)-based\\nfashion recommendation system characterized by high accuracy\\nand dynamic adaptation. The LLM model can quickly adjust to user\\nIntegrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations RecSys 2024, October 14–18, 2024, Bari, Italy\\npreferences and new data distributions. Moving forward, our focus\\nwill be ',\n",
       " ' 14–18, 2024, Bari, Italy\\npreferences and new data distributions. Moving forward, our focus\\nwill be on a hybrid approach that combines the outputs of the LLM\\nand a Conventional Recommendation Model (CRM) to form a model\\nensemble. This ensemble will also incorporate multimodal informa-\\ntion, including images and metadata to further develop the system’s\\nability to process and integrate multimodal data, improving the\\naccuracy and personalization of fashion recommendations.\\nACKNOWLEDGMENTS\\nAcknowled',\n",
       " 'ta, improving the\\naccuracy and personalization of fashion recommendations.\\nACKNOWLEDGMENTS\\nAcknowledgements go here. Delete enclosing begin/end markers if\\nthere are no acknowledgements.\\nREFERENCES\\n[1] Ziad Al-Halah, Rainer Stiefelhagen, and Kristen Grauman. 2017. Fashion for-\\nward: Forecasting visual style in fashion. In Proceedings of the IEEE international\\nconference on computer vision . 388–397.\\n[2] Victor Carbune, Hassan Mansoor, Fangyu Liu, Rahul Aralikatte, Gilles Baechler,\\nJindong Chen, a',\n",
       " '.\\n[2] Victor Carbune, Hassan Mansoor, Fangyu Liu, Rahul Aralikatte, Gilles Baechler,\\nJindong Chen, and Abhanshu Sharma. 2024. Chart-based reasoning: Transferring\\ncapabilities from llms to vlms. arXiv preprint arXiv:2403.12596 (2024).\\n[3] Wen-Huang Cheng, Sijie Song, Chieh-Yun Chen, Shintami Chusnul Hidayati,\\nand Jiaying Liu. 2021. Fashion Meets Computer Vision: A Survey. ACM Comput.\\nSurv. 54, 4, Article 72 (jul 2021), 41 pages. https://doi.org/10.1145/3447239\\n[4] Yujuan Ding, Zhihui Lai, PY Mok,',\n",
       " 'rticle 72 (jul 2021), 41 pages. https://doi.org/10.1145/3447239\\n[4] Yujuan Ding, Zhihui Lai, PY Mok, and Tat-Seng Chua. 2023. Computational\\nTechnologies for Fashion Recommendation: A Survey. Comput. Surveys 56, 5\\n(2023), 1–45.\\n[5] Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S Davis. 2017. Learning\\nfashion compatibility with bidirectional lstms. In Proceedings of the 25th ACM\\ninternational conference on Multimedia . 1078–1086.\\n[6] Hyunwoo Hwangbo, Yang Sok Kim, and Kyung Jin Cha. 2018. Recom',\n",
       " 'ference on Multimedia . 1078–1086.\\n[6] Hyunwoo Hwangbo, Yang Sok Kim, and Kyung Jin Cha. 2018. Recommendation\\nsystem development for fashion retail e-commerce.Electronic Commerce Research\\nand Applications 28 (2018), 94–101.\\n[7] Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan\\nWang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang.\\n2023. CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evalua-\\ntion of Large Language Model Generation. ar',\n",
       " 'ling LLM-as-Critic for Effective and Explainable Evalua-\\ntion of Large Language Model Generation. arXiv:2311.18702 [cs.CL]\\n[8] Yen-Liang Lin, Son Tran, and Larry S. Davis. 2020. Fashion Outfit Complementary\\nItem Retrieval. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR). IEEE. https://doi.org/10.1109/cvpr42600.2020.00337\\n[9] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boosting\\ncontent-based recommendation with both open-and closed-source larg',\n",
       " 'iao-Ming Wu. 2024. Once: Boosting\\ncontent-based recommendation with both open-and closed-source large language\\nmodels. In Proceedings of the 17th ACM International Conference on Web Search\\nand Data Mining. 452–461.\\n[10] Xin Liu, Yongbin Sun, Ziwei Liu, and Dahua Lin. 2020. Learning diverse fashion\\ncollocation by neural graph filtering. IEEE Transactions on Multimedia 23 (2020),\\n2894–2901.\\n[11] Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. 2023. Llm-\\nrec: Personalized recomme',\n",
       " 'jia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. 2023. Llm-\\nrec: Personalized recommendation via prompting large language models. arXiv\\npreprint arXiv:2307.15780 (2023).\\n[12] Dongmei Mo, Xingxing Zou, Kaicheng Pang, and Wai Keung Wong. 2023. To-\\nwards private stylists via personalized compatibility learning. Expert Systems\\nwith Applications 219 (2023), 119632.\\n[13] Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and\\nIoannis Kompatsiaris. 2022. Multimodal Qua',\n",
       " ' Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and\\nIoannis Kompatsiaris. 2022. Multimodal Quasi-AutoRegression: Forecasting the\\nvisual popularity of new fashion products. International Journal of Multimedia\\nInformation Retrieval 11, 4 (2022), 717–729.\\n[14] Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, and\\nIoannis Kompatsiaris. 2022. VICTOR: Visual Incompatibility Detection with\\nTransformers and Fashion-specific contrastive pre-training. arXiv preprint\\narXiv:2207.1',\n",
       " 'tection with\\nTransformers and Fashion-specific contrastive pre-training. arXiv preprint\\narXiv:2207.13458 (2022).\\n[15] Rohan Sarkar, Navaneeth Bodla, Mariya I Vasileva, Yen-Liang Lin, Anurag Beni-\\nwal, Alan Lu, and Gerard Medioni. 2023. Outfittransformer: Learning outfit\\nrepresentations for fashion recommendation. In Proceedings of the IEEE/CVF\\nWinter Conference on Applications of Computer Vision . 3601–3609.\\n[16] Maria Anastassia Stefani, Vassilios Stefanis, and John Garofalakis. 2019. CFRS: a\\nt',\n",
       " ' 3601–3609.\\n[16] Maria Anastassia Stefani, Vassilios Stefanis, and John Garofalakis. 2019. CFRS: a\\ntrends-driven collaborative fashion recommendation system. In 2019 10th Inter-\\nnational Conference on Information, Intelligence, Systems and Applications (IISA) .\\nIEEE, 1–4.\\n[17] Mariya I. Vasileva, Bryan A. Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha\\nKumar, and David Forsyth. 2018. Learning Type-Aware Embeddings for Fashion\\nCompatibility. In Proceedings of the European Conference on Computer V',\n",
       " '-Aware Embeddings for Fashion\\nCompatibility. In Proceedings of the European Conference on Computer Vision\\n(ECCV).\\n[18] Mariya I Vasileva, Bryan A Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha\\nKumar, and David Forsyth. 2018. Learning type-aware embeddings for fashion\\ncompatibility. InProceedings of the European conference on computer vision (ECCV).\\n390–405.\\n[19] Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge\\nBelongie. 2015. Learning visual clothing style with het',\n",
       " 'Bell, Julian McAuley, Kavita Bala, and Serge\\nBelongie. 2015. Learning visual clothing style with heterogeneous dyadic co-\\noccurrences. In Proceedings of the IEEE international conference on computer vision.\\n4642–4650.\\n[20] Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and\\nYang Tang. 2023. A brief overview of ChatGPT: The history, status quo and\\npotential future development. IEEE/CAA Journal of Automatica Sinica 10, 5\\n(2023), 1122–1136.\\n[21] Ruobing Xie, Yanlei Liu, Shaol',\n",
       " '. IEEE/CAA Journal of Automatica Sinica 10, 5\\n(2023), 1122–1136.\\n[21] Ruobing Xie, Yanlei Liu, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin.\\n2021. Personalized approximate pareto-efficient recommendation. In Proceedings\\nof the Web Conference 2021. 3839–3849.\\n[22] Siyuan Xing, Qiulei Dong, and Zhanyi Hu. 2022. SCE-Net: Self-and cross-\\nenhancement network for single-view height estimation and semantic segmenta-\\ntion. Remote Sensing 14, 9 (2022), 2252.\\n[23] Fan Yang, Zheng Chen, Ziyan Jiang, E',\n",
       " 'emantic segmenta-\\ntion. Remote Sensing 14, 9 (2022), 2252.\\n[23] Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin\\nLu. 2023. Palr: Personalization aware llms for recommendation. arXiv preprint\\narXiv:2305.07622 (2023).\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca06bf",
   "metadata": {},
   "source": [
    "## Create Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text_model = AutoModel.from_pretrained(model_name)\n",
    "text_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df9f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 384)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def embed_texts(texts):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = text_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# call the function\n",
    "pdf_embeddings = embed_texts(pdf_chunks)\n",
    "pdf_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2723860",
   "metadata": {},
   "source": [
    "## Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e987602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dim = pdf_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim) # Cosine similarity\n",
    "faiss.normalize_L2(pdf_embeddings)\n",
    "\n",
    "index.add(pdf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595f4e9",
   "metadata": {},
   "source": [
    "## Query FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86bf785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, k=5):\n",
    "    q_emb = embed_texts([query])\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, idxs = index.search(q_emb, k)\n",
    "    return [pdf_chunks[i] for i in idxs[0]]\n",
    "\n",
    "\"\"\"Create a function to query FAISS index and retrieve relevant contexts based on PDF chunks.\"\"\"\n",
    "contexts = search_faiss(\"What outfits are suitable for a summer wear?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6917420",
   "metadata": {},
   "source": [
    "## RAG with LLM (Context-Aware Generation)\n",
    "---\n",
    "## Purpose\n",
    "* ### Use retrieved context\n",
    "* ### Prevent hallucination\n",
    "* ### Generate grounded answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c6e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1048fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc268849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for Mac-Safe, Memory-Safe, Disk-Safe Version\n",
    "\n",
    "import os\n",
    "\n",
    "# Put HF cache in temp (auto-cleans, no disk growth)\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/hf_cache\"\n",
    "\n",
    "# Prevent PyTorch MPS from over-allocating unified memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69eb01c",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397d355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c400fb410d46ed9d36c2e57e14e442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a5460a837e4b0fbfdf2fffbdde2bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b0ad255bee47a3aec0a17398fde8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7147310d3b4bd58bf39656bd89c4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 26.49 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m llm_name = \u001b[33m\"\u001b[39m\u001b[33mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m tokenizer = AutoTokenizer.from_pretrained(llm_name)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/modeling_utils.py:5432\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_hqq_or_quark:\n\u001b[32m   5431\u001b[39m     expanded_device_map = expand_device_map(device_map, expected_keys)\n\u001b[32m-> \u001b[39m\u001b[32m5432\u001b[39m     \u001b[43mcaching_allocator_warmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_device_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5434\u001b[39m \u001b[38;5;66;03m# Prepare and compatabilize arguments for serial and parallel shard loading\u001b[39;00m\n\u001b[32m   5435\u001b[39m args_list = [\n\u001b[32m   5436\u001b[39m     (\n\u001b[32m   5437\u001b[39m         shard_file,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5452\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m shard_file \u001b[38;5;129;01min\u001b[39;00m checkpoint_files\n\u001b[32m   5453\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/modeling_utils.py:6105\u001b[39m, in \u001b[36mcaching_allocator_warmup\u001b[39m\u001b[34m(model, expanded_device_map, hf_quantizer)\u001b[39m\n\u001b[32m   6103\u001b[39m     byte_count = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, byte_count - unused_memory)\n\u001b[32m   6104\u001b[39m \u001b[38;5;66;03m# Allocate memory\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6105\u001b[39m _ = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Invalid buffer size: 26.49 GiB"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "llm_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# bnb_config for 4-bit quantization to prevent memory overload\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llm_name,\n",
    "    use_fast=True # use fast tokenizer\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    quantization_config=bnb_config,  # 4-bit quantization\n",
    "    device_map=\"auto\", # uses MPS safely\n",
    "    low_cpu_mem_usage=True, # avoid duplicate weights in CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4818ad3",
   "metadata": {},
   "source": [
    "## Build RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd888ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76398ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(contexts, question):\n",
    "    context_block = \"\\n\\n\".join(contexts)\n",
    "    return f\"\"\"\n",
    "You are fashion recommendation assistant expert.\n",
    "\n",
    "Context:\n",
    "{context_block}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f6f39",
   "metadata": {},
   "source": [
    "## Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b46e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      7\u001b[39m     outptus = model.generate(\n\u001b[32m      8\u001b[39m         **inputs,\n\u001b[32m      9\u001b[39m         max_new_token=\u001b[32m200\u001b[39m,\n\u001b[32m     10\u001b[39m         temperature=\u001b[32m0.7\u001b[39m\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(outptus[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRecommend a summer casual outfit for men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgenerate_answer\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_answer\u001b[39m(question):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     contexts = \u001b[43msearch_faiss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     prompt = build_prompt(contexts, question=question)\n\u001b[32m      5\u001b[39m     inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36msearch_faiss\u001b[39m\u001b[34m(query, k)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch_faiss\u001b[39m(query, k=\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     q_emb = \u001b[43membed_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     faiss.normalize_L2(q_emb)\n\u001b[32m      4\u001b[39m     scores, idxs = index.search(q_emb, k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36membed_texts\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_texts\u001b[39m(texts):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         inputs = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m         outputs = text_model(**inputs)\n\u001b[32m     10\u001b[39m         embeddings = outputs.last_hidden_state.mean(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2938\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2936\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2937\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2938\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2939\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2940\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3026\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3021\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3022\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3023\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3024\u001b[39m         )\n\u001b[32m   3025\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3044\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3045\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3046\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3047\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3048\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   3049\u001b[39m         text=text,\n\u001b[32m   3050\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3068\u001b[39m         **kwargs,\n\u001b[32m   3069\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3218\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3201\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3202\u001b[39m \u001b[33;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[32m   3203\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3214\u001b[39m \u001b[33;03m        details in `encode_plus`).\u001b[39;00m\n\u001b[32m   3215\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3217\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3218\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3224\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._batch_encode_plus(\n\u001b[32m   3228\u001b[39m     batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3229\u001b[39m     add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3246\u001b[39m     **kwargs,\n\u001b[32m   3247\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2834\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[39m\u001b[34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[39m\n\u001b[32m   2832\u001b[39m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[32m   2833\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy != PaddingStrategy.DO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pad_token_id < \u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2834\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2835\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2836\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2837\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpad_token\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[33m[PAD]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m})`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2838\u001b[39m     )\n\u001b[32m   2840\u001b[39m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[32m   2841\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2842\u001b[39m     truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE\n\u001b[32m   2843\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m padding_strategy != PaddingStrategy.DO_NOT_PAD\n\u001b[32m   (...)\u001b[39m\u001b[32m   2846\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (max_length % pad_to_multiple_of != \u001b[32m0\u001b[39m)\n\u001b[32m   2847\u001b[39m ):\n",
      "\u001b[31mValueError\u001b[39m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "def generate_answer(question):\n",
    "    contexts = search_faiss(question)\n",
    "    prompt = build_prompt(contexts, question=question)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outptus = model.generate(\n",
    "        **inputs,\n",
    "        max_new_token=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return tokenizer.decode(outptus[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_answer(\"Recommend a summer casual outfit for men\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cf33d",
   "metadata": {},
   "source": [
    "## Auto clean mode saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5eccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
